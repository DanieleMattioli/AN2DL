{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks and Deep Learning: Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We run this code on Kaggle kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for random operations. \n",
    "# This let our experiments to be reproducible. \n",
    "SEED = 12\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Set GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/ann-and-dl-vqa/dataset_vqa\n",
      "/kaggle/input/ann-and-dl-vqa/dataset_vqa/train\n"
     ]
    }
   ],
   "source": [
    "# set directories of dataset and test set\n",
    "dataset_dir = os.path.join('/kaggle', 'input')\n",
    "dataset_dir = os.path.join(dataset_dir, 'ann-and-dl-vqa')\n",
    "dataset_dir = os.path.join(dataset_dir, 'dataset_vqa')\n",
    "\n",
    "train_json = os.path.join(dataset_dir, 'train_data.json')\n",
    "test_json = os.path.join(dataset_dir, 'test_data.json')\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "test_dir = os.path.join(dataset_dir, 'test')\n",
    "print(dataset_dir)\n",
    "print(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import json\n",
    "import os.path\n",
    "import random as ra\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, Reshape, Lambda, Embedding, LSTM, Conv2D, MaxPooling2D, TimeDistributed, RepeatVector, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from scipy import ndimage, misc\n",
    "import imageio\n",
    "from PIL import Image\n",
    "from skimage.transform import rotate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We created a custom generator in order to load the data. The generator cycles over the questions and loads the corresponding images.\n",
    "#### The entire dataset could not be loaded at the same time because makes memory explode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Loads & Preprocesses CLEVR dataset.\n",
    "#\n",
    "def load_data(vocab_size, sequence_length):\n",
    "    while True:\n",
    "        # Dataset paths\n",
    "        questions_path = train_json\n",
    "        images_path = train_dir\n",
    "\n",
    "        batch_data = []\n",
    "        x_text = []     # List of questions\n",
    "        x_image = []    # List of images\n",
    "        y = []          # List of answers\n",
    "        num_labels = 13  # Current number of labels, used to create index mapping\n",
    "        labels = {}     # Dictionary mapping of ints to labels\n",
    "        images = {}     # Dictionary of images, to minimize number of imread ops\n",
    "        n = 1081        # Number of data in a batch \n",
    "        tokenizer=None\n",
    "\n",
    "        # Attempt to load saved JSON subset of the questions\n",
    "        #print('Loading data...')\n",
    "\n",
    "        with open(questions_path) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        data = data['questions']\n",
    "        batch_data.append(ra.sample(data,n))\n",
    "        labels = {'0': 0, '1': 1, '10': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, 'no': 11,'yes': 12}\n",
    "\n",
    "        # Store image data and labels in dictionaries\n",
    "\n",
    "        for q in batch_data[0][0:n]:\n",
    "            # Create an index for each image\n",
    "            if not q['image_filename'] in images:\n",
    "\n",
    "                images[q['image_filename']] = imageio.imread(os.path.join(images_path, q['image_filename']), pilmode='RGB')\n",
    "\n",
    "            x_text.append(q['question'])\n",
    "            x_image.append(images[q['image_filename']])\n",
    "            y.append(labels[q['answer']])\n",
    "        # Convert question corpus into sequential encoding for LSTM\n",
    "\n",
    "    \n",
    "        tokenizer = Tokenizer(num_words=vocab_size)\n",
    "\n",
    "        tokenizer.fit_on_texts(x_text)\n",
    "        sequences = tokenizer.texts_to_sequences(x_text)\n",
    "        x_text = sequence.pad_sequences(sequences, maxlen=sequence_length)\n",
    "\n",
    "        # Convert x_image to np array\n",
    "        x_image = np.array(x_image)\n",
    "\n",
    "        # Convert labels to categorical labels\n",
    "        y = keras.utils.to_categorical(y, num_labels) \n",
    "\n",
    "        yield ([x_text, x_image], y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#\n",
    "# Preprocesses the input image by cropping.\n",
    "#\n",
    "def process_image(x):\n",
    "    target_height, target_width = 128, 128\n",
    "    x = tf.image.resize(x, (target_height, target_width), method=tf.image.ResizeMethod.AREA)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Returns relation vectors from an input convolution tensor map.\n",
    "# A relation vector is the concatenation of two objects, \n",
    "#     in this case the objects are \"pixels\" of the tensor.\n",
    "#\n",
    "\n",
    "def get_relation_vectors(x):\n",
    "    objects = []\n",
    "    relations = []\n",
    "    shape = K.int_shape(x)\n",
    "    k = 25     # Hyperparameter which controls how many objects are considered\n",
    "    keys = []\n",
    "\n",
    "    # Get k unique random objects\n",
    "    while k > 0:\n",
    "    i = ra.randint(0, shape[1] - 1)\n",
    "    j = ra.randint(0, shape[2] - 1)\n",
    "\n",
    "        if not (i, j) in keys:\n",
    "            keys.append((i, j))\n",
    "            objects.append(x[:, i, j, :])\n",
    "            k -= 1\n",
    "\n",
    "    # Concatenate each pair of objects to form a relation vector\n",
    "    for i in range(len(objects)):\n",
    "        for j in range(i, len(objects)):\n",
    "            relations.append(K.concatenate([objects[i], objects[j]], axis=1))\n",
    "\n",
    "    # Restack objects into Keras tensor [batch, relation_ID, relation_vectors]\n",
    "    return K.permute_dimensions(K.stack([r for r in relations], axis=0), [1, 0, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Environment Parameters\n",
    "#\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = .00025\n",
    "vocab_size = 1024\n",
    "sequence_length = 64\n",
    "img_rows, img_cols = 320, 480\n",
    "image_input_shape = (img_rows, img_cols, 3)\n",
    "num_labels = 13\n",
    "\n",
    "#\n",
    "# Load & Preprocess CLEVR\n",
    "train_gen = load_data(vocab_size, sequence_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following code allows us to split the dataset in training set and validation set. However the performances with the validation set  were worse than those without the validation set. Thus, we decided not to use the validation set in all our experiments (and we were actually able to reach higher scores on Kaggle).\n",
    "#### This is probably due to class imbalance in the training set with respect to the test set. Therefore the error on the validation set, having the same distribution as the training set, could not be trusted as an estimate of the realt test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_path = train_json\n",
    "with open(questions_path) as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "data = np.array(data['questions'])\n",
    "\n",
    "train_test_split = 0.8\n",
    "train_mask = np.random.choice([True,False], samples, p=[train_test_split, 1-train_test_split])\n",
    "\n",
    "valid_mask = np.logical_not(train_mask)\n",
    "\n",
    "data_train = data[train_mask]\n",
    "data_valid = data[valid_mask]\n",
    "\n",
    "train_gen = load_data_generator(samples, data_train, batch_size, vocab_size, sequence_length)\n",
    "valid_gen = load_data_generator(samples, data_valid, valid_batch_size, vocab_size, sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We implemented the model described in the paper \"A simple neural network module for relational reasoning\" Adam Santoro, David Raposo, David G.T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap https://arxiv.org/pdf/1706.01427.pdf\n",
    "\n",
    "#### The architecture of the model in shown below.\n",
    "\n",
    "#### It is constituted by: \n",
    "    - an LSTM for text analysis\n",
    "    - a CNN for image features extraction\n",
    "    - a Relation Network\n",
    "    - a Fully Connected Classification Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define LSTM\n",
    "#\n",
    "text_inputs = Input(shape=(sequence_length,), name='text_input')\n",
    "text_x = Embedding(vocab_size, 128)(text_inputs)\n",
    "text_x = LSTM(128)(text_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define CNN\n",
    "#\n",
    "image_inputs = Input(shape=image_input_shape, name='image_input')\n",
    "image_x = Lambda(process_image)(image_inputs)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "shape = K.int_shape(image_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define Relation Network layer\n",
    "#\n",
    "RN_inputs = Input(shape=(1, (2 * shape[3]) + K.int_shape(text_x)[1]))\n",
    "RN_x = Dense(256, activation='relu')(RN_inputs)\n",
    "RN_x = Dense(256, activation='relu')(RN_x)\n",
    "RN_x = Dense(256, activation='relu')(RN_x)\n",
    "RN_x = Dropout(.5)(RN_x)\n",
    "RN_outputs = Dense(256, activation='relu')(RN_x)\n",
    "RN = Model(inputs=RN_inputs, outputs=RN_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Implements g_theta\n",
    "#\n",
    "relations = Lambda(get_relation_vectors)(image_x)           # Get tensor [batch, relation_ID, relation_vectors]\n",
    "question = RepeatVector(K.int_shape(relations)[1])(text_x)  # Shape question vector to same size as relations\n",
    "relations = Concatenate(axis=2)([relations, question])      # Merge tensors [batch, relation_ID, relation_vectors, question_vector]\n",
    "g = TimeDistributed(RN)(relations)                          # TimeDistributed applies RN to relation vectors.\n",
    "g = Lambda(lambda x: K.sum(x, axis=1))(g)                   # Sum over relation_ID\n",
    "\n",
    "#\n",
    "# Define f_phi\n",
    "#\n",
    "f = Dense(256, activation='relu')(g)\n",
    "f = Dropout(.5)(f)\n",
    "f = Dense(256, activation='relu')(f)\n",
    "f = Dropout(.5)(f)\n",
    "outputs = Dense(num_labels, activation='softmax')(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Train model\n",
    "#\n",
    "model = Model(inputs=[text_inputs, image_inputs], outputs=outputs)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model was trained with categorical crossentropy loss since the model wants to be able to choose the correct anwser among those of a finite set (each of the answers corresponds to a class, so the task is a classification problem)\n",
    "#### We tried different batch sizes and a different number of epochs and steps per epoch. We found out that using all data in the dataset was leading to lower results on the test set, because of overfitting. Therefore we decided to adopt the following measures:\n",
    "    - train on a dataset (adjusting batch size and steps per epoch)\n",
    "    - train on fewer epochs (since we couldn't perform early stopping)\n",
    "    - add regularization to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "model.fit_generator(train_gen,\n",
    "            epochs=5, \n",
    "            steps_per_epoch=240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Among many others, we tried the following models in order to see how accuracy changed:\n",
    "#### Model 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define LSTM\n",
    "#\n",
    "text_inputs = Input(shape=(sequence_length,), name='text_input')\n",
    "text_x = Embedding(vocab_size, 128)(text_inputs)\n",
    "text_x = LSTM(128)(text_x)\n",
    "\n",
    "#\n",
    "# Define CNN\n",
    "#\n",
    "image_inputs = Input(shape=image_input_shape, name='image_input')\n",
    "image_x = Lambda(process_image)(image_inputs)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "shape = K.int_shape(image_x)\n",
    "\n",
    "#\n",
    "# Define Relation Network layer\n",
    "#\n",
    "RN_inputs = Input(shape=(1, (2 * shape[3]) + K.int_shape(text_x)[1]))\n",
    "RN_x = Dense(256, activation='relu')(RN_inputs)\n",
    "RN_x = Dense(256, activation='relu')(RN_x)\n",
    "RN_x = Dense(256, activation='relu')(RN_x)\n",
    "RN_x = Dropout(.5)(RN_x)\n",
    "RN_outputs = Dense(256, activation='relu')(RN_x)\n",
    "RN = Model(inputs=RN_inputs, outputs=RN_outputs)\n",
    "\n",
    "#\n",
    "# Implements g_theta\n",
    "#\n",
    "relations = Lambda(get_relation_vectors)(image_x)           # Get tensor [batch, relation_ID, relation_vectors]\n",
    "question = RepeatVector(K.int_shape(relations)[1])(text_x)  # Shape question vector to same size as relations\n",
    "relations = Concatenate(axis=2)([relations, question])      # Merge tensors [batch, relation_ID, relation_vectors, question_vector]\n",
    "g = TimeDistributed(RN)(relations)                          # TimeDistributed applies RN to relation vectors.\n",
    "g = Lambda(lambda x: K.sum(x, axis=1))(g)                   # Sum over relation_ID\n",
    "\n",
    "#lowering dropout in f_phi\n",
    "# Define f_phi\n",
    "#\n",
    "f = Dense(256, activation='relu')(g)\n",
    "f = Dropout(.3)(f)\n",
    "f = Dense(256, activation='relu')(f)\n",
    "f = Dropout(.3)(f)\n",
    "outputs = Dense(num_labels, activation='softmax')(f)\n",
    "\n",
    "# results after 5 epochs of training:\n",
    "# accuracy: 0.4114\n",
    "# score on Kaggle: 0.38333\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define LSTM\n",
    "#\n",
    "text_inputs = Input(shape=(sequence_length,), name='text_input')\n",
    "text_x = Embedding(vocab_size, 128)(text_inputs)\n",
    "text_x = LSTM(128)(text_x)\n",
    "\n",
    "#\n",
    "# Define CNN\n",
    "#\n",
    "# increased number of filters at each layer\n",
    "#\n",
    "image_inputs = Input(shape=image_input_shape, name='image_input')\n",
    "image_x = Lambda(process_image)(image_inputs)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(32, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(48, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(64, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "shape = K.int_shape(image_x)\n",
    "\n",
    "#\n",
    "# Define Relation Network layer\n",
    "#\n",
    "# lowering dropout probability\n",
    "#\n",
    "RN_inputs = Input(shape=(1, (2 * shape[3]) + K.int_shape(text_x)[1]))\n",
    "RN_x = Dense(256, activation='relu')(RN_inputs)\n",
    "RN_x = Dense(256, activation='relu')(RN_x)\n",
    "RN_x = Dense(256, activation='relu')(RN_x)\n",
    "RN_x = Dropout(.3)(RN_x)\n",
    "RN_outputs = Dense(256, activation='relu')(RN_x)\n",
    "RN = Model(inputs=RN_inputs, outputs=RN_outputs)\n",
    "\n",
    "#\n",
    "# Implements g_theta\n",
    "#\n",
    "relations = Lambda(get_relation_vectors)(image_x)           # Get tensor [batch, relation_ID, relation_vectors]\n",
    "question = RepeatVector(K.int_shape(relations)[1])(text_x)  # Shape question vector to same size as relations\n",
    "relations = Concatenate(axis=2)([relations, question])      # Merge tensors [batch, relation_ID, relation_vectors, question_vector]\n",
    "g = TimeDistributed(RN)(relations)                          # TimeDistributed applies RN to relation vectors.\n",
    "g = Lambda(lambda x: K.sum(x, axis=1))(g)                   # Sum over relation_ID\n",
    "\n",
    "#\n",
    "# Define f_phi\n",
    "#\n",
    "f = Dense(256, activation='relu')(g)\n",
    "f = Dropout(.3)(f)\n",
    "f = Dense(256, activation='relu')(f)\n",
    "f = Dropout(.3)(f)\n",
    "outputs = Dense(num_labels, activation='softmax')(f)\n",
    "\n",
    "# score on Kaggle: 0.37966"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define LSTM\n",
    "#\n",
    "text_inputs = Input(shape=(sequence_length,), name='text_input')\n",
    "text_x = Embedding(vocab_size, 128)(text_inputs)\n",
    "text_x = LSTM(128)(text_x)\n",
    "\n",
    "#\n",
    "# Define CNN\n",
    "#\n",
    "# increased number of filters at each layer\n",
    "#\n",
    "image_inputs = Input(shape=image_input_shape, name='image_input')\n",
    "image_x = Lambda(process_image)(image_inputs)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(32, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(64, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(128, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "shape = K.int_shape(image_x)\n",
    "#\n",
    "# Define Relation Network layer\n",
    "#\n",
    "# increased dropout probability\n",
    "#\n",
    "RN_inputs = Input(shape=(1, (2 * shape[3]) + K.int_shape(text_x)[1]))\n",
    "RN_x = Dense(256, activation='relu')(RN_inputs)\n",
    "RN_x = Dense(256, activation='relu')(RN_x)\n",
    "RN_x = Dense(256, activation='relu')(RN_x)\n",
    "RN_x = Dropout(.5)(RN_x)\n",
    "RN_outputs = Dense(256, activation='relu')(RN_x)\n",
    "RN = Model(inputs=RN_inputs, outputs=RN_outputs)\n",
    "#\n",
    "# Implements g_theta\n",
    "#\n",
    "relations = Lambda(get_relation_vectors)(image_x)           # Get tensor [batch, relation_ID, relation_vectors]\n",
    "question = RepeatVector(K.int_shape(relations)[1])(text_x)  # Shape question vector to same size as relations\n",
    "relations = Concatenate(axis=2)([relations, question])      # Merge tensors [batch, relation_ID, relation_vectors, question_vector]\n",
    "g = TimeDistributed(RN)(relations)                          # TimeDistributed applies RN to relation vectors.\n",
    "g = Lambda(lambda x: K.sum(x, axis=1))(g)                   # Sum over relation_ID\n",
    "\n",
    "#\n",
    "# Define f_phi\n",
    "#\n",
    "# increased dropout probability \n",
    "#\n",
    "f = Dense(256, activation='relu')(g)\n",
    "f = Dropout(.5)(f)\n",
    "f = Dense(256, activation='relu')(f)\n",
    "f = Dropout(.5)(f)\n",
    "outputs = Dense(num_labels, activation='softmax')(f)\n",
    "\n",
    "# results after 5 epochs of training:\n",
    "# accuracy: 0.3947 \n",
    "# score on Kaggle:0.37866"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define LSTM\n",
    "#\n",
    "text_inputs = Input(shape=(sequence_length,), name='text_input')\n",
    "text_x = Embedding(vocab_size, 128)(text_inputs)\n",
    "text_x = Dropout(0.2)(text_x)\n",
    "text_x = LSTM(128)(text_x)\n",
    "\n",
    "#\n",
    "# Define CNN\n",
    "#\n",
    "image_inputs = Input(shape=image_input_shape, name='image_input')\n",
    "image_x = Lambda(process_image)(image_inputs)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(32, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(48, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(128, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "shape = K.int_shape(image_x)\n",
    "\n",
    "#\n",
    "# Define Relation Network layer\n",
    "#\n",
    "RN_inputs = Input(shape=(1, (2 * shape[3]) + K.int_shape(text_x)[1]))\n",
    "RN_x = Dense(256, activation='relu')(RN_inputs)\n",
    "RN_x = Dense(256, activation='relu')(RN_x)\n",
    "RN_x = Dense(256, activation='relu')(RN_x)\n",
    "RN_x = Dropout(.3)(RN_x)\n",
    "RN_x = Dense(256, activation='relu')(RN_x)\n",
    "RN_x = Dense(256, activation='relu')(RN_x)\n",
    "RN_x = Dropout(.3)(RN_x)\n",
    "RN_outputs = Dense(256, activation='relu')(RN_x)\n",
    "RN = Model(inputs=RN_inputs, outputs=RN_outputs)\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Implements g_theta\n",
    "#\n",
    "relations = Lambda(get_relation_vectors)(image_x)           # Get tensor [batch, relation_ID, relation_vectors]\n",
    "question = RepeatVector(K.int_shape(relations)[1])(text_x)  # Shape question vector to same size as relations\n",
    "relations = Concatenate(axis=2)([relations, question])      # Merge tensors [batch, relation_ID, relation_vectors, question_vector]\n",
    "g = TimeDistributed(RN)(relations)                          # TimeDistributed applies RN to relation vectors.\n",
    "g = Lambda(lambda x: K.sum(x, axis=1))(g)                   # Sum over relation_ID\n",
    "\n",
    "#\n",
    "# Define f_phi\n",
    "#\n",
    "f = Dense(256, activation='relu')(g)\n",
    "f = Dropout(.3)(f)\n",
    "f = Dense(256, activation='relu')(f)\n",
    "f = Dropout(.3)(f)\n",
    "outputs = Dense(num_labels, activation='softmax')(f)\n",
    "\n",
    "# results after 8 epochs of training and n = 512 (n in funciton load_data):\n",
    "# accuracy: 0.4436\n",
    "# score on Kaggle : 0.44333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define LSTM\n",
    "#\n",
    "text_inputs = Input(shape=(sequence_length,), name='text_input')\n",
    "text_x = Embedding(vocab_size, 128)(text_inputs)\n",
    "text_x = Dropout(0.2)(text_x)\n",
    "text_x = LSTM(128)(text_x)\n",
    "\n",
    "#\n",
    "# Define CNN\n",
    "#\n",
    "# increased number of filters at each layer\n",
    "#\n",
    "image_inputs = Input(shape=image_input_shape, name='image_input')\n",
    "image_x = Lambda(process_image)(image_inputs)\n",
    "image_x = Conv2D(32, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(64, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(128, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(256, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "shape = K.int_shape(image_x)\n",
    "#\n",
    "# Define Relation Network layer\n",
    "#\n",
    "# increased dropout probability\n",
    "#\n",
    "RN_inputs = Input(shape=(1, (2 * shape[3]) + K.int_shape(text_x)[1]))\n",
    "RN_x = Dense(256, activation='relu')(RN_inputs)\n",
    "RN_x = Dense(256, activation='relu')(RN_x)\n",
    "RN_x = Dense(256, activation='relu')(RN_x)\n",
    "RN_x = Dropout(.5)(RN_x)\n",
    "RN_outputs = Dense(256, activation='relu')(RN_x)\n",
    "RN = Model(inputs=RN_inputs, outputs=RN_outputs)\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Implements g_theta\n",
    "#\n",
    "relations = Lambda(get_relation_vectors)(image_x)           # Get tensor [batch, relation_ID, relation_vectors]\n",
    "question = RepeatVector(K.int_shape(relations)[1])(text_x)  # Shape question vector to same size as relations\n",
    "relations = Concatenate(axis=2)([relations, question])      # Merge tensors [batch, relation_ID, relation_vectors, question_vector]\n",
    "g = TimeDistributed(RN)(relations)                          # TimeDistributed applies RN to relation vectors.\n",
    "g = Lambda(lambda x: K.sum(x, axis=1))(g)                   # Sum over relation_ID\n",
    "\n",
    "#\n",
    "# Define f_phi\n",
    "#\n",
    "# increased dropout probability \n",
    "#\n",
    "f = Dense(256, activation='relu')(g)\n",
    "f = Dropout(.5)(f)\n",
    "f = Dense(256, activation='relu')(f)\n",
    "f = Dropout(.5)(f)\n",
    "outputs = Dense(num_labels, activation='softmax')(f)\n",
    "\n",
    "# results after 16 epochs of training and n = 300 (n in funciton load_data):\n",
    "# accuracy: 0.4217\n",
    "# score on Kaggle : 0.46733"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following is the testing of our model (to be loaded on Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def create_csv(results, results_dir='./'):\n",
    "\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
    "\n",
    "        f.write('Id,Category\\n')\n",
    "\n",
    "        for key, value in results.items():\n",
    "            f.write(str(key) + ',' + str(value) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to load the data from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_test(n, vocab_size, sequence_length):\n",
    "    questions_path = test_json\n",
    "    images_path = test_dir\n",
    "    x_text = []     # List of questions\n",
    "    x_image = []    # List of images\n",
    "    x_id = []       # Question id \n",
    "    y = []          # List of answers\n",
    "    num_labels = 0  # Current number of labels, used to create index mapping\n",
    "    labels = {}     # Dictionary mapping of ints to labels\n",
    "    images = {}     # Dictionary of images, to minimize number of imread ops\n",
    "\n",
    "    # Attempt to load saved JSON subset of the questions\n",
    "    print('Loading data...')\n",
    "        \n",
    "    with open(questions_path) as f:\n",
    "        data = json.load(f)\n",
    "    data = data['questions'][0:n]\n",
    "    \n",
    "    for q in data[0:n]:\n",
    "        # Create an index for each image\n",
    "        if not q['image_filename'] in images:\n",
    "            images[q['image_filename']] = imageio.imread(os.path.join(images_path, q['image_filename']), pilmode=\"RGB\")\n",
    "\n",
    "        x_text.append(q['question'])\n",
    "        x_image.append(images[q['image_filename']])\n",
    "        x_id.append(q['question_id'])\n",
    "        \n",
    "    # Convert question corpus into sequential encoding for LSTM\n",
    "    print('Processing text data...')\n",
    "    tokenizer = Tokenizer(num_words=vocab_size)\n",
    "\n",
    "    tokenizer.fit_on_texts(x_text)\n",
    "    sequences = tokenizer.texts_to_sequences(x_text)\n",
    "    x_text = sequence.pad_sequences(sequences, maxlen=64)\n",
    "\n",
    "    # Convert x_image to np array\n",
    "    x_image = np.array(x_image)\n",
    "\n",
    "    print('Text: ', x_text.shape)\n",
    "    print('Image: ', x_image.shape)\n",
    "\n",
    "    return [x_text, x_image, x_id], num_labels, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "(texts, images, ids), _, _ = load_data_test(3000, vocab_size, sequence_length)\n",
    "out_softmax = model.predict([texts, images])\n",
    "prediction = tf.math.argmax(out_softmax, axis=-1)   # predicted class\n",
    "print(\"prediction:\")\n",
    "print(prediction)\n",
    "print(prediction[0].numpy())\n",
    "print(prediction[1].numpy())\n",
    "i = 0\n",
    "for id in ids:\n",
    "    results[id] = prediction[i].numpy()\n",
    "    i = i+1\n",
    "create_csv(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
