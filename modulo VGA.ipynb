{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for random operations. \n",
    "# This let our experiments to be reproducible. \n",
    "SEED = 12\n",
    "tf.random.set_seed(SEED)\n",
    "#np.random.seed(SEED)\n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Set GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join('/kaggle', 'input')\n",
    "dataset_dir = os.path.join(dataset_dir, 'ann-and-dl-vqa')\n",
    "dataset_dir = os.path.join(dataset_dir, 'dataset_vqa')\n",
    "\n",
    "train_json = os.path.join(dataset_dir, 'train_data.json')\n",
    "test_json = os.path.join(dataset_dir, 'test_data.json')\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "test_dir = os.path.join(dataset_dir, 'test')\n",
    "print(dataset_dir)\n",
    "print(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import json\n",
    "import os.path\n",
    "import random as ra\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, Reshape, Lambda, Embedding, LSTM, Conv2D, MaxPooling2D, TimeDistributed, RepeatVector, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from scipy import ndimage, misc\n",
    "import imageio\n",
    "from PIL import Image\n",
    "from skimage.transform import rotate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Loads & Preprocesses CLEVR dataset.\n",
    "#\n",
    "def load_data(vocab_size, sequence_length):\n",
    "    while True:\n",
    "        # Dataset paths\n",
    "        questions_path = train_json\n",
    "        images_path = train_dir\n",
    "\n",
    "        batch_data = []\n",
    "        x_text = []     # List of questions\n",
    "        x_image = []    # List of images\n",
    "        y = []          # List of answers\n",
    "        num_labels = 13  # Current number of labels, used to create index mapping\n",
    "        labels = {}     # Dictionary mapping of ints to labels\n",
    "        images = {}     # Dictionary of images, to minimize number of imread ops\n",
    "        n = 1082\n",
    "        tokenizer=None\n",
    "\n",
    "        # Attempt to load saved JSON subset of the questions\n",
    "        #print('Loading data...')\n",
    "\n",
    "        with open(questions_path) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        data = data['questions']\n",
    "        batch_data.append(ra.sample(data,n))\n",
    "        labels = {'0': 0, '1': 1, '10': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, 'no': 11,'yes': 12}\n",
    "\n",
    "        # Store image data and labels in dictionaries\n",
    "\n",
    "        for q in batch_data[0][0:n]:\n",
    "            # Create an index for each image\n",
    "            if not q['image_filename'] in images:\n",
    "\n",
    "                images[q['image_filename']] = imageio.imread(os.path.join(images_path, q['image_filename']), pilmode='RGB')\n",
    "\n",
    "            x_text.append(q['question'])\n",
    "            x_image.append(images[q['image_filename']])\n",
    "            y.append(labels[q['answer']])\n",
    "        # Convert question corpus into sequential encoding for LSTM\n",
    "\n",
    "    \n",
    "        tokenizer = Tokenizer(num_words=vocab_size)\n",
    "\n",
    "        tokenizer.fit_on_texts(x_text)\n",
    "        sequences = tokenizer.texts_to_sequences(x_text)\n",
    "        x_text = sequence.pad_sequences(sequences, maxlen=sequence_length)\n",
    "\n",
    "        # Convert x_image to np array\n",
    "        x_image = np.array(x_image)\n",
    "\n",
    "        # Convert labels to categorical labels\n",
    "        y = keras.utils.to_categorical(y, num_labels) \n",
    "\n",
    "        yield ([x_text, x_image], y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#\n",
    "# Preprocesses the input image by cropping and random rotations.\n",
    "#\n",
    "def process_image(x):\n",
    "    target_height, target_width = 128, 128\n",
    "    rotation_range = .05  # In radians\n",
    "    degs = ra.uniform(-rotation_range, rotation_range)\n",
    "    degs = degs * (180/ math.pi)\n",
    "\n",
    "    x = tf.image.resize(x, (target_height, target_width), method=tf.image.ResizeMethod.AREA)\n",
    "    #x = tf.contrib.image.rotate(x, degs)\n",
    "    #rotate(x, degs)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Returns relation vectors from an input convolution tensor map.\n",
    "# A relation vector is the concatenation of two objects, \n",
    "#     in this case the objects are \"pixels\" of the tensor.\n",
    "#\n",
    "\n",
    "def get_relation_vectors(x):\n",
    "    objects = []\n",
    "    relations = []\n",
    "    shape = K.int_shape(x)\n",
    "    k = 25     # Hyperparameter which controls how many objects are considered\n",
    "    keys = []\n",
    "\n",
    "    # Get k unique random objects\n",
    "    while k > 0:\n",
    "    i = ra.randint(0, shape[1] - 1)\n",
    "    j = ra.randint(0, shape[2] - 1)\n",
    "\n",
    "        if not (i, j) in keys:\n",
    "            keys.append((i, j))\n",
    "            objects.append(x[:, i, j, :])\n",
    "            k -= 1\n",
    "\n",
    "    # Concatenate each pair of objects to form a relation vector\n",
    "    for i in range(len(objects)):\n",
    "        for j in range(i, len(objects)):\n",
    "            relations.append(K.concatenate([objects[i], objects[j]], axis=1))\n",
    "\n",
    "    # Restack objects into Keras tensor [batch, relation_ID, relation_vectors]\n",
    "    return K.permute_dimensions(K.stack([r for r in relations], axis=0), [1, 0, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Environment Parameters\n",
    "#\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = .00025\n",
    "vocab_size = 1024\n",
    "sequence_length = 64\n",
    "img_rows, img_cols = 320, 480\n",
    "image_input_shape = (img_rows, img_cols, 3)\n",
    "num_labels = 13\n",
    "\n",
    "#\n",
    "# Load & Preprocess CLEVR\n",
    "#\n",
    "#(x_train, y_train), num_labels, tokenizer = load_data(samples, vocab_size, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define LSTM\n",
    "#\n",
    "text_inputs = Input(shape=(sequence_length,), name='text_input')\n",
    "text_x = Embedding(vocab_size, 128)(text_inputs)\n",
    "text_x = LSTM(128)(text_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define CNN\n",
    "#\n",
    "image_inputs = Input(shape=image_input_shape, name='image_input')\n",
    "image_x = Lambda(process_image)(image_inputs)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "shape = K.int_shape(image_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define Relation Network layer\n",
    "#\n",
    "RN_inputs = Input(shape=(1, (2 * shape[3]) + K.int_shape(text_x)[1]))\n",
    "RN_x = Dense(256, activation='relu')(RN_inputs)\n",
    "RN_x = Dense(256, activation='relu')(RN_x)\n",
    "RN_x = Dense(256, activation='relu')(RN_x)\n",
    "RN_x = Dropout(.5)(RN_x)\n",
    "RN_outputs = Dense(256, activation='relu')(RN_x)\n",
    "RN = Model(inputs=RN_inputs, outputs=RN_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Implements g_theta\n",
    "#\n",
    "relations = Lambda(get_relation_vectors)(image_x)           # Get tensor [batch, relation_ID, relation_vectors]\n",
    "question = RepeatVector(K.int_shape(relations)[1])(text_x)  # Shape question vector to same size as relations\n",
    "relations = Concatenate(axis=2)([relations, question])      # Merge tensors [batch, relation_ID, relation_vectors, question_vector]\n",
    "g = TimeDistributed(RN)(relations)                          # TimeDistributed applies RN to relation vectors.\n",
    "g = Lambda(lambda x: K.sum(x, axis=1))(g)                   # Sum over relation_ID\n",
    "\n",
    "#\n",
    "# Define f_phi\n",
    "#\n",
    "f = Dense(256, activation='relu')(g)\n",
    "f = Dropout(.5)(f)\n",
    "f = Dense(256, activation='relu')(f)\n",
    "f = Dropout(.5)(f)\n",
    "outputs = Dense(num_labels, activation='softmax')(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Train model\n",
    "#\n",
    "model = Model(inputs=[text_inputs, image_inputs], outputs=outputs)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.compat.v1 import ConfigProto\n",
    "#from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "#config = ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "\n",
    "model.fit(load_data(vocab_size, sequence_length), \n",
    "          steps_per_epoch=240,\n",
    "          epochs=5,\n",
    "          shuffle=True,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def create_csv(results, results_dir='./'):\n",
    "\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
    "\n",
    "        f.write('Id,Category\\n')\n",
    "\n",
    "        for key, value in results.items():\n",
    "            f.write(str(key) + ',' + str(value) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_test(n, vocab_size, sequence_length):\n",
    "    questions_path = test_json\n",
    "    images_path = test_dir\n",
    "    # vocab_size = 13\n",
    "    # n = 6629        # <-- 6629 ??\n",
    "    x_text = []     # List of questions\n",
    "    x_image = []    # List of images\n",
    "    x_id = []\n",
    "    y = []          # List of answers\n",
    "    num_labels = 0  # Current number of labels, used to create index mapping\n",
    "    labels = {}     # Dictionary mapping of ints to labels\n",
    "    images = {}     # Dictionary of images, to minimize number of imread ops\n",
    "\n",
    "    # Attempt to load saved JSON subset of the questions\n",
    "    print('Loading data...')\n",
    "        \n",
    "    with open(questions_path) as f:\n",
    "        data = json.load(f)\n",
    "    data = data['questions'][0:n]\n",
    "    \n",
    "    for q in data[0:n]:\n",
    "        # Create an index for each image\n",
    "        if not q['image_filename'] in images:\n",
    "            images[q['image_filename']] = imageio.imread(os.path.join(images_path, q['image_filename']), pilmode=\"RGB\")\n",
    "\n",
    "        x_text.append(q['question'])\n",
    "        x_image.append(images[q['image_filename']])\n",
    "        x_id.append(q['question_id'])\n",
    "        \n",
    "    # Convert question corpus into sequential encoding for LSTM\n",
    "    print('Processing text data...')\n",
    "    tokenizer = Tokenizer(num_words=vocab_size)\n",
    "\n",
    "    tokenizer.fit_on_texts(x_text)\n",
    "    sequences = tokenizer.texts_to_sequences(x_text)\n",
    "    x_text = sequence.pad_sequences(sequences, maxlen=64) #maxlen era a 100\n",
    "\n",
    "    # Convert x_image to np array\n",
    "    x_image = np.array(x_image)\n",
    "\n",
    "    print('Text: ', x_text.shape)\n",
    "    print('Image: ', x_image.shape)\n",
    "\n",
    "    return [x_text, x_image, x_id], num_labels, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "(texts, images, ids), _, _ = load_data_test(3000, vocab_size, sequence_length)\n",
    "out_softmax = model.predict([texts, images])\n",
    "prediction = tf.math.argmax(out_softmax, axis=-1)   # predicted class\n",
    "print(\"prediction:\")\n",
    "print(prediction)\n",
    "print(prediction[0].numpy())\n",
    "print(prediction[1].numpy())\n",
    "i = 0\n",
    "for id in ids:\n",
    "    results[id] = prediction[i].numpy()\n",
    "    i = i+1\n",
    "#results[ids] = prediction[0].numpy()\n",
    "create_csv(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
