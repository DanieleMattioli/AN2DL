{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for random operations. \n",
    "# This let our experiments to be reproducible. \n",
    "SEED = 12\n",
    "tf.random.set_seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Set GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/ann-and-dl-vqa/dataset_vqa\n",
      "/kaggle/input/ann-and-dl-vqa/dataset_vqa/train\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = os.path.join('/kaggle', 'input')\n",
    "dataset_dir = os.path.join(dataset_dir, 'ann-and-dl-vqa')\n",
    "dataset_dir = os.path.join(dataset_dir, 'dataset_vqa')\n",
    "\n",
    "train_json = os.path.join(dataset_dir, 'train_data.json')\n",
    "test_json = os.path.join(dataset_dir, 'test_data.json')\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "test_dir = os.path.join(dataset_dir, 'test')\n",
    "print(dataset_dir)\n",
    "print(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import json\n",
    "import os.path\n",
    "import random as ra\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, Reshape, Lambda, Embedding, LSTM, Conv2D, MaxPooling2D, TimeDistributed, RepeatVector, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from scipy import ndimage, misc\n",
    "import imageio\n",
    "from PIL import Image\n",
    "from skimage.transform import rotate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Loads & Preprocesses CLEVR dataset.\n",
    "#\n",
    "def load_data_generator(n, data, batch_size, vocab_size, sequence_length, tokenizer=None):\n",
    "    \n",
    "    while(True):\n",
    "        # Dataset paths\n",
    "        images_path = train_dir\n",
    "\n",
    "        x_text = []     # List of questions\n",
    "        x_image = []    # List of images\n",
    "        y = []          # List of answers\n",
    "        num_labels = 13  # Current number of labels, used to create index mapping\n",
    "        labels = {}     # Dictionary mapping of ints to labels\n",
    "        images = {}     # Dictionary of images, to minimize number of imread ops\n",
    "\n",
    "        # Attempt to load saved JSON subset of the questions\n",
    "#         print('Loading data...')\n",
    "\n",
    "        labels = {'0': 0, '1': 1, '10': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, 'no': 11,'yes': 12}\n",
    "\n",
    "        # Store image data and labels in dictionaries\n",
    "#         print('Storing image data...')\n",
    "\n",
    "        batch_paths = np.random.choice(a = data, size = batch_size)\n",
    "\n",
    "        for q in batch_paths:\n",
    "            # Create an index for each image\n",
    "            if not q['image_filename'] in images:\n",
    "\n",
    "                images[q['image_filename']] = imageio.imread(os.path.join(images_path, q['image_filename']), pilmode='RGB')\n",
    "\n",
    "            question = q['question']\n",
    "            x_text.append(question)\n",
    "            #print(\"q[question]\")\n",
    "            #print(q['question'])\n",
    "\n",
    "            image = images[q['image_filename']]\n",
    "            x_image.append(image)\n",
    "            #print(\"q[image_filename]\")\n",
    "            #print(images[q['image_filename']])\n",
    "\n",
    "            label = labels[q['answer']]\n",
    "            y.append(label)\n",
    "            #print(\"q[answer]\")\n",
    "            #print(labels[q['answer']])\n",
    "\n",
    "        # Convert question corpus into sequential encoding for LSTM\n",
    "#         print('Processing text data...')\n",
    "\n",
    "        if not tokenizer:\n",
    "            tokenizer = Tokenizer(num_words=vocab_size)\n",
    "\n",
    "        tokenizer.fit_on_texts(x_text)\n",
    "        sequences = tokenizer.texts_to_sequences(x_text)\n",
    "        x_text = sequence.pad_sequences(sequences, maxlen=sequence_length)\n",
    "\n",
    "        # Convert x_image to np array\n",
    "        x_image = np.array(x_image)\n",
    "\n",
    "        # Convert labels to categorical labels\n",
    "        y = keras.utils.to_categorical(y, num_labels) \n",
    "\n",
    "#         print('Text: ', x_text.shape)\n",
    "#         print('Image: ', x_image.shape)\n",
    "#         print('Labels: ', y.shape)\n",
    "\n",
    "        yield ([x_text, x_image], y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#\n",
    "# Preprocesses the input image by cropping and random rotations.\n",
    "#\n",
    "def process_image(x):\n",
    "    target_height, target_width = 128, 128\n",
    "    rotation_range = .05  # In radians\n",
    "    degs = ra.uniform(-rotation_range, rotation_range)\n",
    "    degs = degs * (180/ math.pi)\n",
    "\n",
    "    x = tf.image.resize(x, (target_height, target_width), method=tf.image.ResizeMethod.AREA)\n",
    "    #x = tf.contrib.image.rotate(x, degs)\n",
    "    #rotate(x, degs)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Returns relation vectors from an input convolution tensor map.\n",
    "# A relation vector is the concatenation of two objects, \n",
    "#     in this case the objects are \"pixels\" of the tensor.\n",
    "#\n",
    "\n",
    "def get_relation_vectors(x):\n",
    "\tobjects = []\n",
    "\trelations = []\n",
    "\tshape = K.int_shape(x)\n",
    "\tk = 25     # Hyperparameter which controls how many objects are considered\n",
    "\tkeys = []\n",
    "\n",
    "\t# Get k unique random objects\n",
    "\twhile k > 0:\n",
    "\t\ti = ra.randint(0, shape[1] - 1)\n",
    "\t\tj = ra.randint(0, shape[2] - 1)\n",
    "\n",
    "\t\tif not (i, j) in keys:\n",
    "\t\t\tkeys.append((i, j))\n",
    "\t\t\tobjects.append(x[:, i, j, :])\n",
    "\t\t\tk -= 1\n",
    "\n",
    "\t# Concatenate each pair of objects to form a relation vector\n",
    "\tfor i in range(len(objects)):\n",
    "\t\tfor j in range(i, len(objects)):\n",
    "\t\t\trelations.append(K.concatenate([objects[i], objects[j]], axis=1))\n",
    "\n",
    "\t# Restack objects into Keras tensor [batch, relation_ID, relation_vectors]\n",
    "\treturn K.permute_dimensions(K.stack([r for r in relations], axis=0), [1, 0, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Environment Parameters\n",
    "#\n",
    "samples = 259492\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "valid_batch_size = 32\n",
    "learning_rate = .00025\n",
    "vocab_size = 1024\n",
    "sequence_length = 64\n",
    "img_rows, img_cols = 320, 480\n",
    "image_input_shape = (img_rows, img_cols, 3)\n",
    "num_labels = 13\n",
    "\n",
    "#\n",
    "# Load & Preprocess CLEVR\n",
    "#\n",
    "# (x_train, y_train), num_labels, tokenizer = load_data_generator(samples, batch_size, vocab_size, sequence_length)\n",
    "\n",
    "\n",
    "questions_path = train_json\n",
    "with open(questions_path) as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "data = np.array(data['questions'])\n",
    "    \n",
    "# print(data)\n",
    "# print(len(data['questions']))\n",
    "\n",
    "train_test_split = 0.8\n",
    "train_mask = np.random.choice([True,False], samples, p=[train_test_split, 1-train_test_split])\n",
    "\n",
    "valid_mask = np.logical_not(train_mask)\n",
    "\n",
    "data_train = data[train_mask]\n",
    "data_valid = data[valid_mask]\n",
    "\n",
    "train_gen = load_data_generator(samples, data_train, batch_size, vocab_size, sequence_length)\n",
    "valid_gen = load_data_generator(samples, data_valid, valid_batch_size, vocab_size, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define LSTM\n",
    "#\n",
    "text_inputs = Input(shape=(sequence_length,), name='text_input')\n",
    "text_x = Embedding(vocab_size, 128)(text_inputs)\n",
    "text_x = LSTM(128)(text_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define CNN\n",
    "#\n",
    "image_inputs = Input(shape=image_input_shape, name='image_input')\n",
    "image_x = Lambda(process_image)(image_inputs)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "shape = K.int_shape(image_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define Relation Network layer\n",
    "#\n",
    "RN_inputs = Input(shape=(1, (2 * shape[3]) + K.int_shape(text_x)[1]))\n",
    "RN_x = Dense(256, activation='relu')(RN_inputs)\n",
    "RN_x = Dense(256, activation='relu')(RN_x)\n",
    "RN_x = Dense(256, activation='relu')(RN_x)\n",
    "RN_x = Dropout(.5)(RN_x)\n",
    "RN_outputs = Dense(256, activation='relu')(RN_x)\n",
    "RN = Model(inputs=RN_inputs, outputs=RN_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Implements g_theta\n",
    "#\n",
    "relations = Lambda(get_relation_vectors)(image_x)           # Get tensor [batch, relation_ID, relation_vectors]\n",
    "question = RepeatVector(K.int_shape(relations)[1])(text_x)  # Shape question vector to same size as relations\n",
    "relations = Concatenate(axis=2)([relations, question])      # Merge tensors [batch, relation_ID, relation_vectors, question_vector]\n",
    "g = TimeDistributed(RN)(relations)                          # TimeDistributed applies RN to relation vectors.\n",
    "g = Lambda(lambda x: K.sum(x, axis=1))(g)                   # Sum over relation_ID\n",
    "\n",
    "#\n",
    "# Define f_phi\n",
    "#\n",
    "f = Dense(256, activation='relu')(g)\n",
    "f = Dropout(.5)(f)\n",
    "f = Dense(256, activation='relu')(f)\n",
    "f = Dropout(.5)(f)\n",
    "outputs = Dense(num_labels, activation='softmax')(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image_input (InputLayer)        (None, 320, 480, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 128, 128, 3)  0           image_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 63, 63, 24)   672         lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 63, 63, 24)   96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 31, 31, 24)   5208        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 31, 31, 24)   96          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 15, 15, 24)   5208        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 15, 15, 24)   96          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "text_input (InputLayer)         (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 7, 7, 24)     5208        batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 64, 128)      131072      text_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 7, 7, 24)     96          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 128)          131584      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 325, 48)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 325, 128)     0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 325, 176)     0           lambda_2[0][0]                   \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 325, 256)     242688      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 256)          0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          65792       lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          65792       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 13)           3341        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 656,949\n",
      "Trainable params: 656,757\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Train model\n",
    "#\n",
    "model = Model(inputs=[text_inputs, image_inputs], outputs=outputs)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  9/240 [>.............................] - ETA: 7:36 - loss: 36.1080 - accuracy: 0.1181"
     ]
    }
   ],
   "source": [
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "\n",
    "model.fit_generator(train_gen,\n",
    "            epochs=100, #epochs, \n",
    "            steps_per_epoch=240,\n",
    "            validation_data=valid_gen,\n",
    "            validation_steps=1,\n",
    "            callbacks=[EarlyStopping(monitor='val_loss', patience=5)])\n",
    "\n",
    "\n",
    "\n",
    "# model.fit(x_train, y_train, \n",
    "#           batch_size=batch_size, \n",
    "#           epochs=20, #epochs, \n",
    "#           shuffle=True,\n",
    "#           callbacks=[EarlyStopping(monitor='val_loss', patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def create_csv(results, results_dir='./'):\n",
    "\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
    "\n",
    "        f.write('Id,Category\\n')\n",
    "        print('Id,Category\\n')\n",
    "\n",
    "        for key, value in results.items():\n",
    "            f.write(str(key) + ',' + str(value) + '\\n')\n",
    "            print(str(key) + ',' + str(value) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_test(n, vocab_size, sequence_length):\n",
    "    questions_path = test_json\n",
    "    images_path = test_dir\n",
    "    # vocab_size = 13\n",
    "    # n = 6629        # <-- 6629 ??\n",
    "    x_text = []     # List of questions\n",
    "    x_image = []    # List of images\n",
    "    x_id = []\n",
    "    y = []          # List of answers\n",
    "    num_labels = 0  # Current number of labels, used to create index mapping\n",
    "    labels = {}     # Dictionary mapping of ints to labels\n",
    "    images = {}     # Dictionary of images, to minimize number of imread ops\n",
    "\n",
    "    # Attempt to load saved JSON subset of the questions\n",
    "    print('Loading data...')\n",
    "        \n",
    "    with open(questions_path) as f:\n",
    "        data = json.load(f)\n",
    "    data = data['questions'][0:n]\n",
    "    \n",
    "    for q in data[0:n]:\n",
    "        # Create an index for each image\n",
    "        if not q['image_filename'] in images:\n",
    "            images[q['image_filename']] = imageio.imread(os.path.join(images_path, q['image_filename']), pilmode=\"RGB\")\n",
    "\n",
    "        x_text.append(q['question'])\n",
    "        x_image.append(images[q['image_filename']])\n",
    "        x_id.append(q['question_id'])\n",
    "        \n",
    "    # Convert question corpus into sequential encoding for LSTM\n",
    "    print('Processing text data...')\n",
    "    tokenizer = Tokenizer(num_words=vocab_size)\n",
    "\n",
    "    tokenizer.fit_on_texts(x_text)\n",
    "    sequences = tokenizer.texts_to_sequences(x_text)\n",
    "    x_text = sequence.pad_sequences(sequences, maxlen=64) #maxlen era a 100\n",
    "\n",
    "    # Convert x_image to np array\n",
    "    x_image = np.array(x_image)\n",
    "\n",
    "    print('Text: ', x_text.shape)\n",
    "    print('Image: ', x_image.shape)\n",
    "\n",
    "    return [x_text, x_image, x_id], num_labels, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "(texts, images, ids), _, _ = load_data_test(3000, vocab_size, sequence_length)\n",
    "out_softmax = model.predict([texts, images])\n",
    "prediction = tf.math.argmax(out_softmax, axis=-1)   # predicted class\n",
    "print(\"prediction:\")\n",
    "print(prediction)\n",
    "print(prediction[0].numpy())\n",
    "print(prediction[1].numpy())\n",
    "i = 0\n",
    "for id in ids:\n",
    "    results[id] = prediction[i].numpy()\n",
    "    i = i+1\n",
    "#results[ids] = prediction[0].numpy()\n",
    "create_csv(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
